{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Metacritic Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "movie_name = \"black or white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CHARS_TO_REMOVE = \"[\\:\\;,\\.'/\\!]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _parse_name_for_search(movie_name):\n",
    "    parsed = re.sub(CHARS_TO_REMOVE, '', movie_name)\n",
    "    return parsed.replace(' ', '+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SEARCH_URL = \"http://www.metacritic.com/search/all/{movie_name}/results?cats%5Bmovie%5D=1&search_type=advanced\"\n",
    "_HEADERS = {'User-Agent': 'Mozilla/5.0'}\n",
    "METACRITIC_URL = \"http://www.metacritic.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = SEARCH_URL.format(movie_name=_parse_name_for_search(movie_name))\n",
    "request = urllib.request.Request(query, headers=_HEADERS)\n",
    "search_res = bs(urllib.request.urlopen(request), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = search_res.find_all(\"li\", {\"class\": \"result\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black Cat, White Cat\n",
      "False\n",
      "False\n",
      "Black or White\n",
      "True\n",
      "True\n",
      "Black and White\n",
      "False\n",
      "False\n",
      "White King, Red Rubber, Black Death\n",
      "False\n",
      "False\n",
      "Family Portrait in Black and White\n",
      "False\n",
      "False\n",
      "Herblock: The Black & the White\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    title = result.find_all(\"h3\", {\"class\": \"product_title\"})[0].contents[0].contents[0]\n",
    "    print(title)\n",
    "    title_match = title.strip().lower() == movie_name.strip().lower()\n",
    "    print(title_match)\n",
    "    year_match = str(2015) in str(result)\n",
    "    print(year_match)\n",
    "    if title_match and year_match:\n",
    "        correct_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first_res = search_res.find_all(\"li\", {\"class\": \"result first_result\"})[0]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-129-e8679a2e6bec>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-129-e8679a2e6bec>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    return METACRITIC_URL + movie_url_suffix\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "movie_url_suffix = correct_result.find_all(\"a\")[0]['href']\n",
    "return METACRITIC_URL + movie_url_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.metacritic.com/movie/black-or-white'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METACRITIC_URL + movie_url_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _get_movie_url_by_name(movie_name):\n",
    "    query = SEARCH_URL.format(movie_name=_parse_name_for_search(movie_name))\n",
    "    request = urllib.request.Request(query, headers=_HEADERS)\n",
    "    search_res = bs(urllib.request.urlopen(request), \"html.parser\")\n",
    "    first_res = search_res.find_all(\"li\", {\"class\": \"result first_result\"})[0]\n",
    "    movie_url_suffix = first_res.find_all(\"a\")[0]['href']\n",
    "    return METACRITIC_URL + movie_url_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.metacritic.com/movie/kingsman-the-secret-service'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_url = _get_movie_url_by_name(movie_name)\n",
    "movie_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Critics Reviews Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CRITICS_REVIEWS_URL_SUFFIX = \"/critic-reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "critics_url = movie_url + CRITICS_REVIEWS_URL_SUFFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "critics_request = urllib.request.Request(critics_url, headers=_HEADERS)\n",
    "critics_page = bs(urllib.request.urlopen(critics_request), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SCORE_CLASSES = [\n",
    "    \"metascore_w larger movie positive\",\n",
    "    \"metascore_w larger movie mixed\",\n",
    "    \"metascore_w larger movie negative\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metascore = int(critics_page.find_all(\"span\", {\"class\": SCORE_CLASSES})[0].contents[0])\n",
    "metascore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MONTH_SHORTHAND_MAP = {\n",
    "    \"Jan\": \"January\", \"Feb\": \"February\", \"Mar\": \"March\", \"Apr\": \"April\",\n",
    "    \"May\": \"May\", \"Jun\": \"June\", \"Jul\": \"July\", \"Aug\": \"August\",\n",
    "    \"Sep\": \"September\", \"Oct\": \"October\", \"Nov\": \"November\", \"Dec\": \"December\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _parse_date_str(date_str):\n",
    "    for month in MONTH_SHORTHAND_MAP:\n",
    "        if month in date_str:\n",
    "            return date_str.replace(month, MONTH_SHORTHAND_MAP[month])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _get_critic_review_props(review):\n",
    "    review_props = {}\n",
    "    date_str = review.find_all(\"span\", {\"class\": \"date\"})[0].contents[0]\n",
    "    date_str = _parse_date_str(date_str)\n",
    "    review_props['review_date'] = datetime.strptime(date_str, \"%B %d, %Y\").date()\n",
    "    review_props['score'] = int(review.find_all(\"div\", {\"class\": \"metascore_w\"})[0].contents[0])\n",
    "    review_props['summary'] = review.find_all('a', {'class': 'no_hover'})[0].contents[0].strip()\n",
    "    review_props['publication'] = None\n",
    "    review_props['critic'] = None\n",
    "    for link in review.find_all(\"a\"):\n",
    "        if 'publication' in link['href']:\n",
    "            review_props['publication'] = link.contents[0]\n",
    "        if 'critic' in link['href']:\n",
    "            review_props['critic'] = link.contents[0]\n",
    "    return review_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for review in critics_page.find_all(\"div\", {\"class\": \"review\"}):\n",
    "    try:\n",
    "        reviews.append(_get_critic_review_props(review))\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## User Reviews Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _get_user_review_props(review):\n",
    "    review_props = {}\n",
    "    date_str = review.find_all(\"span\", {\"class\": \"date\"})[0].contents[0]\n",
    "    date_str = _parse_date_str(date_str)\n",
    "    review_props['review_date'] = datetime.strptime(date_str, \"%B %d, %Y\").date()\n",
    "    review_props['score'] = int(review.find_all(\"div\", {\"class\": \"metascore_w\"})[0].contents[0])\n",
    "    try:\n",
    "        review_props['text'] = review.find_all('span', {'class': 'blurb blurb_expanded'})[0].contents[0].strip()\n",
    "    except IndexError:\n",
    "        review_props['text'] = review.find_all('div', {'class': 'review_body'})[0].contents[1].contents[0].strip()\n",
    "    review_props['user'] = review.find_all('span', {'class': 'author'})[0].contents[0].contents[0]\n",
    "    review_props['total_reactions'] = int(review.find_all('span', {'class': 'total_count'})[0].contents[0])\n",
    "    review_props['pos_reactions'] = int(review.find_all('span', {'class': 'yes_count'})[0].contents[0])\n",
    "    review_props['neg_reactions'] = review_props['total_reactions'] - review_props['pos_reactions']\n",
    "    return review_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "USERS_REVIEWS_URL_SUFFIX = \"/user-reviews?page=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "USER_SCORE_CLASSES = [\n",
    "    \"metascore_w user larger movie positive\",\n",
    "    \"metascore_w user larger movie mixed\",\n",
    "    \"metascore_w user larger movie negative\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _get_user_rating_freq(users_page, rating):\n",
    "    return int(users_page.find_all(\"div\", {\"class\": \"chart {}\".format(rating)})[0].find_all(\n",
    "            \"div\", {\"class\": \"count fr\"})[0].contents[0].replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _get_user_reviews_from_page(users_page):\n",
    "    review_elements = users_page.find_all(\"div\", {\"class\": \"review\"})\n",
    "    user_reviews = []\n",
    "    for review in review_elements:\n",
    "        try:\n",
    "            user_reviews.append(_get_user_review_props(review))\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(\"Extracted {} reviews.\".format(len(user_reviews)))\n",
    "    nexts = users_page.find_all(\"a\", {\"class\": \"action\", \"rel\": \"next\"})\n",
    "    if len(nexts) > 0: \n",
    "        next_url = METACRITIC_URL + nexts[0]['href']\n",
    "        next_request = urllib.request.Request(next_url, headers=_HEADERS)\n",
    "        next_page = bs(urllib.request.urlopen(next_request), \"html.parser\")\n",
    "        user_reviews += _get_user_reviews_from_page(next_page)\n",
    "    return user_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _get_user_reviews_props(movie_url):\n",
    "    users_url = movie_url + USERS_REVIEWS_URL_SUFFIX\n",
    "    users_request = urllib.request.Request(users_url, headers=_HEADERS)\n",
    "    users_page = bs(urllib.request.urlopen(users_request), \"html.parser\")\n",
    "    users_props = {}\n",
    "    user_score = float(users_page.find_all(\"span\", {\"class\": USER_SCORE_CLASSES})[0].contents[0])\n",
    "    users_props['user_score'] = user_score\n",
    "    for rating in ['positive', 'mixed', 'negative']:\n",
    "        users_props['{}_rating_frequency'.format(rating)] = _get_user_rating_freq(users_page, rating)\n",
    "    users_props['user_reviews'] = _get_user_reviews_from_page(users_page)\n",
    "    return users_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 100 reviews.\n",
      "Extracted 100 reviews.\n",
      "Extracted 20 reviews.\n"
     ]
    }
   ],
   "source": [
    "users_props = _get_user_reviews_props(movie_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "<meta property=\"og:title\" content=\"Kingsman: The Secret Service\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kingsman: The Secret Service'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_page.find_all(\"meta\", {\"property\": \"og:title\"})[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_reviews', 'mixed_rating_frequency', 'user_score', 'negative_rating_frequency', 'positive_rating_frequency'])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_props.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 100 reviews.\n",
      "Extracted 100 reviews.\n",
      "Extracted 20 reviews.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Uniting profiles to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import morejson as json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROF_DIR_PATH = '/Users/shaypalachy/clones/rotten_needles/data/metacritic_profiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMDB_PROF_DIR_PATH = '/Users/shaypalachy/clones/rotten_needles/data/imdb_profiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading .DS_Store\n",
      "Reading 13_sins.json\n",
      "Reading 1915.json\n",
      "Reading 22_jump_street.json\n",
      "Reading 300_rise_of_an_empire.json\n",
      "Reading 3_days_to_kill.json\n",
      "Reading 50_to_1.json\n",
      "Reading a_haunted_house_2.json\n",
      "Reading a_merry_friggin_christmas.json\n",
      "Reading a_million_ways_to_die_in_the_west.json\n",
      "Reading a_walk_among_the_tombstones.json\n",
      "Reading about_last_night.json\n",
      "Reading accidental_love.json\n",
      "Reading addicted.json\n",
      "Reading adult_beginners.json\n",
      "Reading adult_world.json\n",
      "Reading after_the_dark.json\n",
      "Reading agent_47.json\n",
      "Reading alexander_and_the_terrible_horrible_no_good_very_bad_day.json\n",
      "Reading alien_abduction.json\n",
      "Reading aloha.json\n",
      "Reading america_imagine_the_world_without_her.json\n",
      "Reading and_so_it_goes.json\n",
      "Reading android_cop.json\n",
      "Reading annabelle.json\n",
      "Reading annie.json\n",
      "Reading ant-man.json\n",
      "Reading apocalypse_pompeii.json\n",
      "Reading as_above_so_below.json\n",
      "Reading avengers_age_of_ultron.json\n",
      "Reading back_in_the_day.json\n",
      "Reading bad_johnson.json\n",
      "Reading barefoot.json\n",
      "Reading better_living_through_chemistry.json\n",
      "Reading beyond_the_lights.json\n",
      "Reading beyond_the_reach.json\n",
      "Reading big_eyes.json\n",
      "Reading big_hero_6.json\n",
      "Reading black_mass.json\n",
      "Reading black_or_white.json\n",
      "Reading black_water_vampire.json\n",
      "Reading blackhat.json\n",
      "Reading blended.json\n",
      "Reading brick_mansions.json\n",
      "Reading cake.json\n",
      "Reading camp_takota.json\n",
      "Reading camp_x-ray.json\n",
      "Reading captain_america_the_winter_soldier.json\n",
      "Reading chappie.json\n",
      "Reading cheap_thrills_(film).json\n",
      "Reading child_44.json\n",
      "Reading cinderella.json\n",
      "Reading cold_comes_the_night.json\n",
      "Reading cold_in_july.json\n",
      "Reading comet.json\n",
      "Reading concussion.json\n",
      "Reading crimson_peak.json\n",
      "Reading cymbeline.json\n",
      "Reading da_sweet_blood_of_jesus.json\n",
      "Reading daddys_home.json\n",
      "Reading danny_collins.json\n",
      "Reading date_and_switch.json\n",
      "Reading dawn_of_the_planet_of_the_apes.json\n",
      "Reading dear_white_people.json\n",
      "Reading decoding_annie_parker.json\n",
      "Reading deliver_us_from_evil.json\n",
      "Reading devils_due.json\n",
      "Reading devils_knot.json\n",
      "Reading do_you_believe.json\n",
      "Reading dolphin_tale_2.json\n",
      "Reading dope.json\n",
      "Reading dracula_untold.json\n",
      "Reading draft_day.json\n",
      "Reading dumb_and_dumber_to.json\n",
      "Reading dumbbells.json\n",
      "Reading earth_to_echo.json\n",
      "Reading endless_love.json\n",
      "Reading entourage.json\n",
      "Reading everest.json\n",
      "Reading everly.json\n",
      "Reading ex_machina.json\n",
      "Reading exodus_gods_and_kings.json\n",
      "Reading far_from_the_madding_crowd.json\n",
      "Reading fifty_shades_of_grey.json\n",
      "Reading focus.json\n",
      "Reading foxcatcher.json\n",
      "Reading furious_7.json\n",
      "Reading fury.json\n",
      "Reading get_hard.json\n",
      "Reading get_on_up.json\n",
      "Reading gods_not_dead.json\n",
      "Reading gods_pocket.json\n",
      "Reading gone_girl.json\n",
      "Reading goodbye_to_all_that.json\n",
      "Reading guardians_of_the_galaxy.json\n",
      "Reading happy_christmas.json\n",
      "Reading heaven_is_for_real.json\n",
      "Reading hellion.json\n",
      "Reading hercules.json\n",
      "Reading home.json\n",
      "Reading home_sweet_hell.json\n",
      "Reading honeymoon.json\n",
      "Reading horrible_bosses_2.json\n",
      "Reading hot_pursuit.json\n",
      "Reading hot_tub_time_machine_2.json\n",
      "Reading how_to_train_your_dragon_2.json\n",
      "Reading i_frankenstein.json\n",
      "Reading i_origins.json\n",
      "Reading if_i_stay.json\n",
      "Reading in_the_blood.json\n",
      "Reading in_the_heart_of_the_sea.json\n",
      "Reading inherent_vice.json\n",
      "Reading inside_out.json\n",
      "Reading insidious_chapter_3.json\n",
      "Reading interstellar.json\n",
      "Reading into_the_storm.json\n",
      "Reading into_the_woods.json\n",
      "Reading it_follows.json\n",
      "Reading jack_ryan_shadow_recruit.json\n",
      "Reading jamesy_boy.json\n",
      "Reading jersey_boys.json\n",
      "Reading jessabelle.json\n",
      "Reading jinn.json\n",
      "Reading joy.json\n",
      "Reading jupiter_ascending.json\n",
      "Reading jurassic_world.json\n",
      "Reading just_before_i_go.json\n",
      "Reading kid_cannabis.json\n",
      "Reading kingsman_the_secret_service.json\n",
      "Reading labor_day.json\n",
      "Reading laggies.json\n",
      "Reading left_behind.json\n",
      "Reading leprechaun_origins.json\n",
      "Reading lets_be_cops.json\n",
      "Reading life_after_beth.json\n",
      "Reading listen_up_philip.json\n",
      "Reading little_boy.json\n",
      "Reading love_&_mercy.json\n",
      "Reading love_is_strange.json\n",
      "Reading love_rosie.json\n",
      "Reading mad_max_fury_road.json\n",
      "Reading maggie.json\n",
      "Reading magic_in_the_moonlight.json\n",
      "Reading maleficent.json\n",
      "Reading maps_to_the_stars.json\n",
      "Reading match.json\n",
      "Reading max.json\n",
      "Reading mcfarland_usa.json\n",
      "Reading me_and_earl_and_the_dying_girl.json\n",
      "Reading million_dollar_arm.json\n",
      "Reading minions.json\n",
      "Reading moms_night_out.json\n",
      "Reading monkey_kingdom.json\n",
      "Reading mortdecai.json\n",
      "Reading mr_peabody_&_sherman.json\n",
      "Reading muppets_most_wanted.json\n",
      "Reading murder_101.json\n",
      "Reading my_man_is_a_loser.json\n",
      "Reading need_for_speed.json\n",
      "Reading night_at_the_museum_secret_of_the_tomb.json\n",
      "Reading nightcrawler.json\n",
      "Reading no_escape.json\n",
      "Reading no_good_deed.json\n",
      "Reading noah.json\n",
      "Reading non-stop.json\n",
      "Reading nurse_3d.json\n",
      "Reading obvious_child.json\n",
      "Reading oculus.json\n",
      "Reading open_grave.json\n",
      "Reading ouija.json\n",
      "Reading out_of_the_dark.json\n",
      "Reading paddington.json\n",
      "Reading paranormal_activity_the_marked_ones.json\n",
      "Reading paul_blart_mall_cop_2.json\n",
      "Reading penguins_of_madagascar.json\n",
      "Reading persecuted.json\n",
      "Reading ping_pong_summer.json\n",
      "Reading pitch_perfect_2.json\n",
      "Reading pixels.json\n",
      "Reading planes_fire_&_rescue.json\n",
      "Reading poltergeist.json\n",
      "Reading project_almanac.json\n",
      "Reading rage.json\n",
      "Reading raze.json\n",
      "Reading rebound.json\n",
      "Reading ride_along.json\n",
      "Reading rio_2.json\n",
      "Reading road_hard.json\n",
      "Reading road_to_paloma.json\n",
      "Reading robocop.json\n",
      "Reading run_all_night.json\n",
      "Reading sabotage.json\n",
      "Reading san_andreas.json\n",
      "Reading saving_christmas.json\n",
      "Reading serena.json\n",
      "Reading seventh_son.json\n",
      "Reading sex_tape.json\n",
      "Reading snowden.json\n",
      "Reading someone_marry_barry.json\n",
      "Reading son_of_god.json\n",
      "Reading song_one.json\n",
      "Reading spare_parts.json\n",
      "Reading spy.json\n",
      "Reading stage_fright.json\n",
      "Reading step_up_all_in.json\n",
      "Reading strange_magic.json\n",
      "Reading tammy.json\n",
      "Reading ted_2.json\n",
      "Reading terminator_genisys.json\n",
      "Reading that_awkward_moment.json\n",
      "Reading the_age_of_adaline.json\n",
      "Reading the_amazing_spider-man_2.json\n",
      "Reading the_angriest_man_in_brooklyn.json\n",
      "Reading the_bag_man.json\n",
      "Reading the_better_angels.json\n",
      "Reading the_boxtrolls.json\n",
      "Reading the_boy_next_door.json\n",
      "Reading the_cobbler.json\n",
      "Reading the_d_train.json\n",
      "Reading the_divergent_series_insurgent.json\n",
      "Reading the_duff.json\n",
      "Reading the_equalizer.json\n",
      "Reading the_expendables_3.json\n",
      "Reading the_fault_in_our_stars.json\n",
      "Reading the_giver.json\n",
      "Reading the_good_dinosaur.json\n",
      "Reading the_good_lie.json\n",
      "Reading the_guest.json\n",
      "Reading the_gunman.json\n",
      "Reading the_homesman.json\n",
      "Reading the_humbling.json\n",
      "Reading the_hundred-foot_journey.json\n",
      "Reading the_interview.json\n",
      "Reading the_judge.json\n",
      "Reading the_lazarus_effect.json\n",
      "Reading the_loft.json\n",
      "Reading the_longest_ride.json\n",
      "Reading the_martian.json\n",
      "Reading the_maze_runner.json\n",
      "Reading the_monuments_men.json\n",
      "Reading the_nut_job.json\n",
      "Reading the_one_i_love.json\n",
      "Reading the_other_woman.json\n",
      "Reading the_outsider.json\n",
      "Reading the_pretty_one.json\n",
      "Reading the_prince.json\n",
      "Reading the_purge_anarchy.json\n",
      "Reading the_revenant.json\n",
      "Reading the_road_within.json\n",
      "Reading the_scribbler.json\n",
      "Reading the_second_best_exotic_marigold_hotel.json\n",
      "Reading the_signal.json\n",
      "Reading the_skeleton_twins.json\n",
      "Reading the_song.json\n",
      "Reading the_spongebob_movie_sponge_out_of_water.json\n",
      "Reading the_two_faces_of_january.json\n",
      "Reading the_vatican_tapes.json\n",
      "Reading the_voices.json\n",
      "Reading the_walking_deceased.json\n",
      "Reading the_wedding_ringer.json\n",
      "Reading the_woman_in_black_2_angel_of_death.json\n",
      "Reading they_came_together.json\n",
      "Reading this_is_where_i_leave_you.json\n",
      "Reading tiger_orange.json\n",
      "Reading tomorrowland.json\n",
      "Reading top_five.json\n",
      "Reading tracers.json\n",
      "Reading trainwreck.json\n",
      "Reading transcendence.json\n",
      "Reading true_story.json\n",
      "Reading tusk.json\n",
      "Reading unbroken.json\n",
      "Reading unfinished_business.json\n",
      "Reading unfriended.json\n",
      "Reading vampire_academy.json\n",
      "Reading veronica_mars.json\n",
      "Reading vice.json\n",
      "Reading victor_frankenstein.json\n",
      "Reading walk_of_shame.json\n",
      "Reading when_marnie_was_there.json\n",
      "Reading while_were_young.json\n",
      "Reading whiplash.json\n",
      "Reading wild.json\n",
      "Reading wild_card.json\n",
      "Reading wish_i_was_here.json\n",
      "Reading witching_hour.json\n",
      "Reading woman_in_gold.json\n",
      "Reading x-men_days_of_future_past.json\n",
      "Reading young_ones.json\n",
      "Reading youre_not_you.json\n"
     ]
    }
   ],
   "source": [
    "profiles = []\n",
    "for profile_file in os.listdir(PROF_DIR_PATH):\n",
    "    print('Reading {}'.format(profile_file))\n",
    "    file_path = os.path.join(PROF_DIR_PATH, profile_file)\n",
    "    file_name, ext = os.path.splitext(file_path)\n",
    "    if ext != '.json':\n",
    "        continue\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        profiles.append(json.load(json_file))\n",
    "df = pd.DataFrame(profiles)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda [ds]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
